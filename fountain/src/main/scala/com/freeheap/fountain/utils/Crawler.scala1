package com.netflix.utils

/**
 * Created by minhdo on 1/23/16.
 */


import java.util.ArrayList
import java.util.List
import java.util.Locale
import edu.uci.ics.crawler4j.crawler.CrawlController
import edu.uci.ics.crawler4j.crawler.Page
import edu.uci.ics.crawler4j.crawler.exceptions.ContentFetchException
import edu.uci.ics.crawler4j.crawler.exceptions.RedirectException
import edu.uci.ics.crawler4j.fetcher.PageFetchResult
import edu.uci.ics.crawler4j.fetcher.PageFetcher
import edu.uci.ics.crawler4j.frontier.DocIDServer
import edu.uci.ics.crawler4j.frontier.Frontier
import edu.uci.ics.crawler4j.parser.ParseData
import edu.uci.ics.crawler4j.parser.Parser
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer
import edu.uci.ics.crawler4j.url.WebURL
import org.apache.http.HttpStatus
import org.apache.http.impl.EnglishReasonPhraseCatalog
import edu.uci.ics.crawler4j.crawler.exceptions.ContentFetchException
import edu.uci.ics.crawler4j.crawler.exceptions.PageBiggerThanMaxSizeException
import edu.uci.ics.crawler4j.crawler.exceptions.ParseException
import edu.uci.ics.crawler4j.crawler.exceptions.RedirectException
import edu.uci.ics.crawler4j.fetcher.PageFetchResult
import edu.uci.ics.crawler4j.fetcher.PageFetcher
import edu.uci.ics.crawler4j.frontier.DocIDServer
import edu.uci.ics.crawler4j.frontier.Frontier
import edu.uci.ics.crawler4j.parser.NotAllowedContentException
import edu.uci.ics.crawler4j.parser.ParseData
import edu.uci.ics.crawler4j.parser.Parser
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer
import edu.uci.ics.crawler4j.url.WebURL
import uk.org.lidalia.slf4jext.Level
import uk.org.lidalia.slf4jext.Logger
import uk.org.lidalia.slf4jext.LoggerFactory


class WebCrawler  {
  protected val logger: Logger = LoggerFactory.getLogger(classOf[WebCrawler])
  /**
   * The id associated to the crawler thread running this instance
   */
  protected var myId: Int = 0
  /**
   * The controller instance that has created this crawler thread. This
   * reference to the controller can be used for getting configurations of the
   * current crawl or adding new seeds during runtime.
   */
  protected var myController: CrawlController = null

  /**
   * The parser that is used by this crawler instance to parse the content of the fetched pages.
   */
  private var parser: Parser = null
  /**
   * The fetcher that is used by this crawler instance to fetch the content of pages from the web.
   */
  private var pageFetcher: PageFetcher = null
  /**
   * The RobotstxtServer instance that is used by this crawler instance to
   * determine whether the crawler is allowed to crawl the content of each page.
   */
  private var robotstxtServer: RobotstxtServer = null
  /**
   * The DocIDServer that is used by this crawler instance to map each URL to a unique docid.
   */
  private var docIdServer: DocIDServer = null
  /**
   * The Frontier object that manages the crawl queue.
   */
  private var frontier: Frontier = null
  /**
   * Is the current crawler instance waiting for new URLs? This field is
   * mainly used by the controller to detect whether all of the crawler
   * instances are waiting for new URLs and therefore there is no more work
   * and crawling can be stopped.
   */
  private var isWaitingForNewURLs: Boolean = false

  /**
   * Initializes the current instance of the crawler
   *
   * @param id
   * the id of this crawler instance
   * @param crawlController
   * the controller that manages this crawling session
   */
  def init(id: Int, crawlController: CrawlController) {
    this.myId = id
    this.pageFetcher = crawlController.getPageFetcher
    this.robotstxtServer = crawlController.getRobotstxtServer
    this.docIdServer = crawlController.getDocIdServer
    this.frontier = crawlController.getFrontier
    this.parser = new Parser(crawlController.getConfig)
    this.myController = crawlController
    this.isWaitingForNewURLs = false
  }

  /**
   * Get the id of the current crawler instance
   *
   * @return the id of the current crawler instance
   */
  def getMyId: Int = {
    return myId
  }

  def getMyController: CrawlController = {
    return myController
  }

  /**
   * This function is called just before starting the crawl by this crawler
   * instance. It can be used for setting up the data structures or
   * initializations needed by this crawler instance.
   */
  def onStart {
  }

  /**
   * This function is called just before the termination of the current
   * crawler instance. It can be used for persisting in-memory data or other
   * finalization tasks.
   */
  def onBeforeExit {
  }

  /**
   * This function is called once the header of a page is fetched. It can be
   * overridden by sub-classes to perform custom logic for different status
   * codes. For example, 404 pages can be logged, etc.
   *
   * @param webUrl WebUrl containing the statusCode
   * @param statusCode Html Status Code number
   * @param statusDescription Html Status COde description
   */
  protected def handlePageStatusCode(webUrl: WebURL, statusCode: Int, statusDescription: String) {
  }

  /**
   * This function is called before processing of the page's URL
   * It can be overridden by subclasses for tweaking of the url before processing it.
   * For example, http://abc.com/def?a=123 - http://abc.com/def
   *
   * @param curURL current URL which can be tweaked before processing
   * @return tweaked WebURL
   */
  protected def handleUrlBeforeProcess(curURL: WebURL): WebURL = {
    return curURL
  }

  /**
   * This function is called if the content of a url is bigger than allowed size.
   *
   * @param urlStr - The URL which it's content is bigger than allowed size
   */
  protected def onPageBiggerThanMaxSize(urlStr: String, pageSize: Long) {
    logger.warn("Skipping a URL: {} which was bigger ( {} ) than max allowed size", urlStr, pageSize)
  }

  /**
   * This function is called if the crawler encountered an unexpected http status code ( a status code other than 3xx)
   *
   * @param urlStr URL in which an unexpected error was encountered while crawling
   * @param statusCode Html StatusCode
   * @param contentType Type of Content
   * @param description Error Description
   */
  protected def onUnexpectedStatusCode(urlStr: String, statusCode: Int, contentType: String, description: String) {
    logger.warn("Skipping URL: {}, StatusCode: {}, {}, {}", urlStr, statusCode, contentType, description)
  }

  /**
   * This function is called if the content of a url could not be fetched.
   *
   * @param webUrl URL which content failed to be fetched
   */
  protected def onContentFetchError(webUrl: WebURL) {
    logger.warn("Can't fetch content of: {}", webUrl.getURL)
  }

  /**
   * This function is called when a unhandled exception was encountered during fetching
   *
   * @param webUrl URL where a unhandled exception occured
   */
  protected def onUnhandledException(webUrl: WebURL, e: Throwable) {
    val urlStr: String = (if (webUrl == null) "NULL" else webUrl.getURL)
    logger.warn("Unhandled exception while fetching {}: {}", urlStr, e.getMessage)
    logger.info("Stacktrace: ", e)
  }

  /**
   * This function is called if there has been an error in parsing the content.
   *
   * @param webUrl URL which failed on parsing
   */
  protected def onParseError(webUrl: WebURL) {
    logger.warn("Parsing error of: {}", webUrl.getURL)
  }

  /**
   * The CrawlController instance that has created this crawler instance will
   * call this function just before terminating this crawler thread. Classes
   * that extend WebCrawler can override this function to pass their local
   * data to their controller. The controller then puts these local data in a
   * List that can then be used for processing the local data of crawlers (if needed).
   *
   * @return currently NULL
   */
  def getMyLocalData: AnyRef = {
    return null
  }

  def run {
    onStart
    while (true) {
      val assignedURLs: List[WebURL] = new ArrayList[WebURL](50)
      isWaitingForNewURLs = true
      frontier.getNextURLs(50, assignedURLs)
      isWaitingForNewURLs = false
      if (assignedURLs.isEmpty) {
        if (frontier.isFinished) {
          return
        }
        try {
          Thread.sleep(3000)
        }
        catch {
          case e: InterruptedException => {
            logger.error("Error occurred", e)
          }
        }
      }
      else {
        import scala.collection.JavaConversions._
        for (curURL <- assignedURLs) {
          if (myController.isShuttingDown) {
            logger.info("Exiting because of controller shutdown.")
            return
          }
          if (curURL != null) {
            val url = handleUrlBeforeProcess(curURL)
            processPage(url)
            frontier.setProcessed(url)
          }
        }
      }
    }
  }

  /**
   * Classes that extends WebCrawler should overwrite this function to tell the
   * crawler whether the given url should be crawled or not. The following
   * default implementation indicates that all urls should be included in the crawl.
   *
   * @param url
   * the url which we are interested to know whether it should be
   * included in the crawl or not.
   * @param referringPage
   * The Page in which this url was found.
   * @return if the url should be included in the crawl it returns true,
   *         otherwise false is returned.
   */
  def shouldVisit(referringPage: Page, url: WebURL): Boolean = {
    return true
  }

  /**
   * Classes that extends WebCrawler should overwrite this function to process
   * the content of the fetched and parsed page.
   *
   * @param page
   * the page object that is just fetched and parsed.
   */
  def visit(page: Page) {
  }

  private def processPage(curURL: WebURL) {
    var fetchResult: PageFetchResult = null
    try {
      if (curURL == null) {
        throw new Exception("Failed processing a NULL url !?")
      }
      fetchResult = pageFetcher.fetchPage(curURL)
      val statusCode: Int = fetchResult.getStatusCode
      handlePageStatusCode(curURL, statusCode, EnglishReasonPhraseCatalog.INSTANCE.getReason(statusCode, Locale.ENGLISH))
      val page: Page = new Page(curURL)
      page.setFetchResponseHeaders(fetchResult.getResponseHeaders)
      page.setStatusCode(statusCode)
      if (statusCode < 200 || statusCode > 299) {
        if (statusCode == HttpStatus.SC_MOVED_PERMANENTLY || statusCode == HttpStatus.SC_MOVED_TEMPORARILY || statusCode == HttpStatus.SC_MULTIPLE_CHOICES || statusCode == HttpStatus.SC_SEE_OTHER || statusCode == HttpStatus.SC_TEMPORARY_REDIRECT || statusCode == 308) {
          page.setRedirect(true)
          if (myController.getConfig.isFollowRedirects) {
            val movedToUrl: String = fetchResult.getMovedToUrl
            if (movedToUrl == null) {
              throw new RedirectException(Level.WARN, "Unexpected error, URL: " + curURL + " is redirected to NOTHING")
            }
            page.setRedirectedToUrl(movedToUrl)
            val newDocId: Int = docIdServer.getDocId(movedToUrl)
            if (newDocId > 0) {
              throw new RedirectException(Level.DEBUG, "Redirect page: " + curURL + " is already seen")
            }
            val webURL: WebURL = new WebURL
            webURL.setURL(movedToUrl)
            webURL.setParentDocid(curURL.getParentDocid)
            webURL.setParentUrl(curURL.getParentUrl)
            webURL.setDepth(curURL.getDepth)
            webURL.setDocid(-1)
            webURL.setAnchor(curURL.getAnchor)
            if (shouldVisit(page, webURL)) {
              if (robotstxtServer.allows(webURL)) {
                webURL.setDocid(docIdServer.getNewDocID(movedToUrl))
                frontier.schedule(webURL)
              }
              else {
                logger.debug("Not visiting: {} as per the server's \"robots.txt\" policy", webURL.getURL)
              }
            }
            else {
              logger.debug("Not visiting: {} as per your \"shouldVisit\" policy", webURL.getURL)
            }
          }
        }
        else {
          val description: String = EnglishReasonPhraseCatalog.INSTANCE.getReason(fetchResult.getStatusCode, Locale.ENGLISH)
          val contentType: String = if (fetchResult.getEntity == null) "" else fetchResult.getEntity.getContentType.getValue
          onUnexpectedStatusCode(curURL.getURL, fetchResult.getStatusCode, contentType, description)
        }
      }
      else {
        if (!(curURL.getURL == fetchResult.getFetchedUrl)) {
          if (docIdServer.isSeenBefore(fetchResult.getFetchedUrl)) {
            throw new RedirectException(Level.DEBUG, "Redirect page: " + curURL + " has already been seen")
          }
          curURL.setURL(fetchResult.getFetchedUrl)
          curURL.setDocid(docIdServer.getNewDocID(fetchResult.getFetchedUrl))
        }
        if (!fetchResult.fetchContent(page)) {
          throw new ContentFetchException
        }
        parser.parse(page, curURL.getURL)
        val parseData: ParseData = page.getParseData
        val toSchedule: List[WebURL] = new ArrayList[WebURL]
        val maxCrawlDepth: Int = myController.getConfig.getMaxDepthOfCrawling
        import scala.collection.JavaConversions._
        for (webURL <- parseData.getOutgoingUrls) {
          webURL.setParentDocid(curURL.getDocid)
          webURL.setParentUrl(curURL.getURL)
          val newdocid: Int = docIdServer.getDocId(webURL.getURL)
          if (newdocid > 0) {
            webURL.setDepth(-1.toShort)
            webURL.setDocid(newdocid)
          }
          else {
            webURL.setDocid(-1)
            webURL.setDepth((curURL.getDepth + 1).toShort)
            if ((maxCrawlDepth == -1) || (curURL.getDepth < maxCrawlDepth)) {
              if (shouldVisit(page, webURL)) {
                if (robotstxtServer.allows(webURL)) {
                  webURL.setDocid(docIdServer.getNewDocID(webURL.getURL))
                  toSchedule.add(webURL)
                }
                else {
                  logger.debug("Not visiting: {} as per the server's \"robots.txt\" policy", webURL.getURL)
                }
              }
              else {
                logger.debug("Not visiting: {} as per your \"shouldVisit\" policy", webURL.getURL)
              }
            }
          }
        }
        frontier.scheduleAll(toSchedule)
        visit(page)
      }
    }
    catch {
      case e: PageBiggerThanMaxSizeException => {
        onPageBiggerThanMaxSize(curURL.getURL, e.getPageSize)
      }
      case pe: ParseException => {
        onParseError(curURL)
      }
      case cfe: ContentFetchException => {
        onContentFetchError(curURL)
      }
      case re: RedirectException => {
        logger.log(re.level, re.getMessage)
      }
      case nace: NotAllowedContentException => {
        logger.debug("Skipping: {} as it contains binary content which you configured not to crawl", curURL.getURL)
      }
      case e: Exception => {
        onUnhandledException(curURL, e)
      }
    } finally {
      if (fetchResult != null) {
        fetchResult.discardContentIfNotConsumed
      }
    }
  }


  def isNotWaitingForNewURLs: Boolean = {
    return !isWaitingForNewURLs
  }
}

